import os
import time
import requests
import re
from io import BytesIO
from bs4 import BeautifulSoup
from PIL import Image, ImageEnhance
from fake_useragent import UserAgent
from dotenv import load_dotenv

class WeixinToXiaohongshu:
    def __init__(self):
        self.headers = {
            'User-Agent': UserAgent().random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        load_dotenv()
        self.xhs_cookie = os.getenv('XHS_COOKIE')
        self.base_save_path = r"E:\fy\æ™ºä¼å†…æ¨\data"
        
    def create_save_directory(self, title):
        """åˆ›å»ºä¿å­˜ç›®å½•"""
        # æ¸…ç†æ ‡é¢˜ä¸­çš„éæ³•å­—ç¬¦
        safe_title = re.sub(r'[\\/:*?"<>|]', '_', title)
        save_dir = os.path.join(self.base_save_path, safe_title)
        
        # åˆ›å»ºç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        os.makedirs(os.path.join(save_dir, 'images'), exist_ok=True)
        
        return save_dir
        
    def save_content(self, save_dir, content, styled_content):
        """ä¿å­˜æ–‡æœ¬å†…å®¹"""
        # ä¿å­˜åŸå§‹å†…å®¹
        with open(os.path.join(save_dir, 'original.txt'), 'w', encoding='utf-8') as f:
            f.write(content)
            
        # ä¿å­˜å°çº¢ä¹¦é£æ ¼å†…å®¹
        with open(os.path.join(save_dir, 'xiaohongshu.txt'), 'w', encoding='utf-8') as f:
            f.write(styled_content)
            
    def save_images(self, save_dir, images):
        """ä¸‹è½½å¹¶ä¿å­˜å›¾ç‰‡"""
        saved_images = []
        for i, img_url in enumerate(images):
            try:
                print(f"æ­£åœ¨ä¸‹è½½ç¬¬ {i+1}/{len(images)} å¼ å›¾ç‰‡...")
                response = requests.get(img_url, headers=self.headers)
                img = Image.open(BytesIO(response.content))
                
                # å¦‚æœå›¾ç‰‡æ˜¯RGBAæ¨¡å¼ï¼Œè½¬æ¢ä¸ºRGB
                if img.mode == 'RGBA':
                    # åˆ›å»ºç™½è‰²èƒŒæ™¯
                    background = Image.new('RGB', img.size, (255, 255, 255))
                    # å°†åŸå›¾å¤åˆ¶åˆ°ç™½è‰²èƒŒæ™¯ä¸Š
                    background.paste(img, mask=img.split()[3])  # ä½¿ç”¨alphaé€šé“ä½œä¸ºmask
                    img = background
                elif img.mode != 'RGB':
                    img = img.convert('RGB')
                
                # å¤„ç†å›¾ç‰‡
                processed_img = self.process_image(img)
                if processed_img:
                    # ä¿å­˜å¤„ç†åçš„å›¾ç‰‡
                    save_path = os.path.join(save_dir, 'images', f'image_{i+1}.jpg')
                    processed_img.save(save_path, 'JPEG', quality=95)
                    saved_images.append(save_path)
                    print(f"å›¾ç‰‡å·²ä¿å­˜: {save_path}")
                    
            except Exception as e:
                print(f"å¤„ç†ç¬¬ {i+1} å¼ å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")
                continue
                
        return saved_images
        
    def process_image(self, img):
        """å¤„ç†å›¾ç‰‡ï¼Œæ·»åŠ å°çº¢ä¹¦é£æ ¼æ»¤é•œ"""
        try:
            # å¢åŠ é¥±å’Œåº¦
            enhancer = ImageEnhance.Color(img)
            img = enhancer.enhance(1.2)
            
            # å¢åŠ å¯¹æ¯”åº¦
            enhancer = ImageEnhance.Contrast(img)
            img = enhancer.enhance(1.1)
            
            # å¢åŠ äº®åº¦
            enhancer = ImageEnhance.Brightness(img)
            img = enhancer.enhance(1.1)
            
            return img
            
        except Exception as e:
            print(f"å¤„ç†å›¾ç‰‡å¤±è´¥: {str(e)}")
            return None
            
    def get_weixin_content(self, url):
        """è·å–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å†…å®¹"""
        try:
            print("æ­£åœ¨è®¿é—®æ–‡ç« é“¾æ¥...")
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            print("è§£ææ–‡ç« å†…å®¹...")
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è·å–æ–‡ç« å†…å®¹
            content_div = soup.find(id="js_content")
            if not content_div:
                raise Exception("æœªæ‰¾åˆ°æ–‡ç« å†…å®¹")
                
            # è·å–æ–‡ç« æ ‡é¢˜
            title = soup.find(class_="rich_media_title").get_text().strip() if soup.find(class_="rich_media_title") else ""
                
            # æå–æ–‡æœ¬å’Œå›¾ç‰‡
            text_content = []
            images = []
            seen_texts = set()  # ç”¨äºå»é‡
            
            # æå–æ–‡æœ¬
            for p in content_div.find_all(['p', 'span']):
                text = p.get_text().strip()
                if text and text not in seen_texts:
                    text_content.append(text)
                    seen_texts.add(text)
                    
            # æå–å›¾ç‰‡
            for img in content_div.find_all('img'):
                img_url = img.get('data-src')
                if img_url and img_url not in images:  # å»é‡
                    images.append(img_url)
                    
            return {
                'title': title,
                'text': '\n'.join(text_content),
                'images': images
            }
            
        except Exception as e:
            print(f"è·å–å¾®ä¿¡æ–‡ç« å¤±è´¥: {str(e)}")
            return None
            
    def convert_to_xhs_style(self, title, content):
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºå°çº¢ä¹¦é£æ ¼"""
        # åˆ†æ®µå¤„ç†
        paragraphs = content.split('\n')
        filtered_paragraphs = []
        seen_paragraphs = set()
        
        for p in paragraphs:
            p = p.strip()
            if p and p not in seen_paragraphs:
                filtered_paragraphs.append(p)
                seen_paragraphs.add(p)
                
        # æ„å»ºå°çº¢ä¹¦é£æ ¼çš„å†…å®¹
        styled_content = []
        
        # æ·»åŠ æ ‡é¢˜ï¼ˆä½¿ç”¨emojiè£…é¥°ï¼‰
        styled_content.append(f"âœ¨ {title} âœ¨")
        styled_content.append("")  # ç©ºè¡Œ
        
        # æ·»åŠ å¼€åœºç™½
        styled_content.append("å¤§å®¶å¥½å‘€~ ä»Šå¤©ç»™å¤§å®¶åˆ†äº«ä¸€ç¯‡è¶…æ£’çš„æ–‡ç«  ğŸŒŸ")
        styled_content.append("")  # ç©ºè¡Œ
        
        # æ·»åŠ æ­£æ–‡
        for i, p in enumerate(filtered_paragraphs):
            if i < 3:  # åªä¿ç•™å‰ä¸‰æ®µä¸»è¦å†…å®¹
                styled_content.append(p)
                
        # æ·»åŠ æ€»ç»“
        styled_content.append("")
        styled_content.append("ğŸ’¡ é‡ç‚¹æ€»ç»“ï¼š")
        styled_content.append("1ï¸âƒ£ " + filtered_paragraphs[0][:100] + "...")
        if len(filtered_paragraphs) > 1:
            styled_content.append("2ï¸âƒ£ " + filtered_paragraphs[1][:100] + "...")
            
        # æ·»åŠ emoji
        emoji_dict = {
            "æ¨è": "ğŸ‘",
            "åˆ†äº«": "ğŸ‰",
            "å–œæ¬¢": "â¤ï¸",
            "å»ºè®®": "ğŸ’¡",
            "æé†’": "âš ï¸",
            "æ³¨æ„": "â—",
            "é‡è¦": "â€¼ï¸",
            "æ¸¸æˆ": "ğŸ®",
            "ç›´æ’­": "ğŸ“±",
            "ç©å®¶": "ğŸ‘¥",
            "æˆæœ¬": "ğŸ’°",
            "è¥é”€": "ğŸ“¢"
        }
        
        # æ·»åŠ æ ‡ç­¾
        tags = [
            "#ç»éªŒåˆ†äº«",
            "#å¹²è´§åˆ†äº«",
            "#æ¯æ—¥ä¸€è¯»",
            "#æ–‡ç« æ¨è",
            "#å¹²è´§å¿…çœ‹"
        ]
        
        # åˆå¹¶å†…å®¹
        final_content = "\n".join(styled_content)
        
        # æ·»åŠ emoji
        for key, emoji in emoji_dict.items():
            final_content = final_content.replace(key, f"{key}{emoji}")
            
        # æ·»åŠ æ ‡ç­¾
        final_content += "\n\n" + " ".join(tags)
        
        return final_content
        
    def process_url(self, url):
        """å¤„ç†å•ä¸ªURL"""
        print(f"\nå¼€å§‹å¤„ç†URL: {url}")
        
        # 1. è·å–æ–‡ç« å†…å®¹
        content = self.get_weixin_content(url)
        if not content:
            print("è·å–æ–‡ç« å¤±è´¥ï¼")
            return False
            
        print(f"\næˆåŠŸè·å–æ–‡ç« ï¼š{content['title']}")
        
        # 2. åˆ›å»ºä¿å­˜ç›®å½•
        save_dir = self.create_save_directory(content['title'])
        print(f"åˆ›å»ºä¿å­˜ç›®å½•ï¼š{save_dir}")
        
        # 3. è½¬æ¢ä¸ºå°çº¢ä¹¦é£æ ¼
        styled_content = self.convert_to_xhs_style(content['title'], content['text'])
        
        # 4. ä¿å­˜æ–‡æœ¬å†…å®¹
        self.save_content(save_dir, content['text'], styled_content)
        print("æ–‡æœ¬å†…å®¹å·²ä¿å­˜")
        
        # 5. ä¸‹è½½å¹¶ä¿å­˜å›¾ç‰‡
        saved_images = self.save_images(save_dir, content['images'])
        print(f"å…±ä¿å­˜ {len(saved_images)} å¼ å›¾ç‰‡")
        
        return True

def main():
    url = input("è¯·è¾“å…¥å¾®ä¿¡å…¬ä¼—å·æ–‡ç« URLï¼š")
    converter = WeixinToXiaohongshu()
    
    if converter.process_url(url):
        print("\nå¤„ç†å®Œæˆï¼")
    else:
        print("\nå¤„ç†å¤±è´¥ï¼")

if __name__ == "__main__":
    main() 